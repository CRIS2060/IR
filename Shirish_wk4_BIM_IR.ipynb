{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from math import log\n"
      ],
      "metadata": {
        "id": "muRIDucxK_t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72EJXpCJdi3Q",
        "outputId": "572ec51e-3877-46f1-a1ec-6b1256a70773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function\n",
        "def preprocess(text):\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower())\n"
      ],
      "metadata": {
        "id": "6q6klgEqLB0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load documents\n",
        "def load_documents(folder_path):\n",
        "    docs = {}\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(folder_path, filename), 'r') as file:\n",
        "                docs[filename] = preprocess(file.read())\n",
        "    return docs\n"
      ],
      "metadata": {
        "id": "Fzq0xK1ELExV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load queries\n",
        "def load_queries(query_file_path):\n",
        "    with open(query_file_path, 'r') as file:\n",
        "        return [line.strip() for line in file.readlines()]\n"
      ],
      "metadata": {
        "id": "QJpzM5L2LH_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute term frequencies and document frequencies\n",
        "def compute_statistics(docs):\n",
        "    doc_count = len(docs)\n",
        "    term_doc_freq = defaultdict(int)\n",
        "    term_freq = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for doc_id, words in docs.items():\n",
        "        word_set = set(words)\n",
        "        for word in words:\n",
        "            term_freq[doc_id][word] += 1\n",
        "        for word in word_set:\n",
        "            term_doc_freq[word] += 1\n",
        "\n",
        "    return term_freq, term_doc_freq, doc_count"
      ],
      "metadata": {
        "id": "esKGI2lnLZZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute relevance probabilities using BIM\n",
        "def compute_relevance_prob(query, term_freq, term_doc_freq, doc_count):\n",
        "    scores = {}\n",
        "    for doc_id in term_freq:\n",
        "        score = 1.0\n",
        "        for term in query:\n",
        "            tf = term_freq[doc_id].get(term, 0)\n",
        "            df = term_doc_freq.get(term, 0)\n",
        "            p_term_given_relevant = (tf + 1) / (sum(term_freq[doc_id].values()) + len(term_doc_freq))\n",
        "            p_term_given_not_relevant = (df + 1) / (doc_count - df + len(term_doc_freq))\n",
        "            score *= (p_term_given_relevant / p_term_given_not_relevant)\n",
        "        scores[doc_id] = score\n",
        "    return scores"
      ],
      "metadata": {
        "id": "u04gsli_Lelc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Main retrieval function\n",
        "def retrieve_documents(folder_path, query_file_path):\n",
        "    docs = load_documents(folder_path)\n",
        "    queries = load_queries(query_file_path)\n",
        "\n",
        "    term_freq, term_doc_freq, doc_count = compute_statistics(docs)\n",
        "\n",
        "    # Define the output file path in the same directory as the dataset folder\n",
        "    output_file_path = os.path.join(folder_path, 'Shirish_result.txt')\n",
        "\n",
        "    # Open a file to write results in the same directory as the dataset\n",
        "    with open(output_file_path, 'w') as result_file:\n",
        "        for query in queries:\n",
        "            query_terms = preprocess(query)\n",
        "            scores = compute_relevance_prob(query_terms, term_freq, term_doc_freq, doc_count)\n",
        "            ranked_docs = sorted(scores.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "            # Print to console and write to file\n",
        "            print(f\"Query: {query}\")\n",
        "            result_file.write(f\"Query: {query}\\n\")\n",
        "\n",
        "            # Print and write only the top 3 results with ranks\n",
        "            for rank, (doc_id, score) in enumerate(ranked_docs[:3], start=1):\n",
        "                print(f\"Rank {rank}: Document {doc_id}, Score: {score:.4f}\")\n",
        "                result_file.write(f\"Rank {rank}: Document {doc_id}, Score: {score:.4f}\\n\")\n",
        "\n",
        "            print()\n",
        "            result_file.write(\"\\n\")\n",
        "\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/info_retrieval_lab/Final project/dataset'\n",
        "query_file_path = '/content/drive/MyDrive/info_retrieval_lab/Final project/queries.txt'\n",
        "retrieve_documents(folder_path, query_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7EO32CGb4P6",
        "outputId": "6f97862d-aaf5-4057-aa20-3862d6777cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: {\\rtf1\\ansi\\ansicpg1252\\cocoartf2709\n",
            "Rank 1: Document Great_Dane.txt, Score: 0.4478\n",
            "Rank 2: Document Labrador.txt, Score: 0.4127\n",
            "Rank 3: Document Beagle.txt, Score: 0.3810\n",
            "\n",
            "Query: \\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\froman\\fcharset0 Times-Roman;}\n",
            "Rank 1: Document Great_Dane.txt, Score: 0.2005\n",
            "Rank 2: Document Labrador.txt, Score: 0.1703\n",
            "Rank 3: Document Beagle.txt, Score: 0.1452\n",
            "\n",
            "Query: {\\colortbl;\\red255\\green255\\blue255;\\red0\\green0\\blue0;}\n",
            "Rank 1: Document Great_Dane.txt, Score: 0.2451\n",
            "Rank 2: Document Labrador.txt, Score: 0.2125\n",
            "Rank 3: Document Beagle.txt, Score: 0.1848\n",
            "\n",
            "Query: {\\*\\expandedcolortbl;;\\cssrgb\\c0\\c0\\c0;}\n",
            "Rank 1: Document Great_Dane.txt, Score: 0.3663\n",
            "Rank 2: Document Labrador.txt, Score: 0.3308\n",
            "Rank 3: Document Beagle.txt, Score: 0.2993\n",
            "\n",
            "Query: \\paperw11900\\paperh16840\\margl1440\\margr1440\\vieww11520\\viewh8400\\viewkind0\n",
            "Rank 1: Document Great_Dane.txt, Score: 0.2451\n",
            "Rank 2: Document Labrador.txt, Score: 0.2125\n",
            "Rank 3: Document Beagle.txt, Score: 0.1848\n",
            "\n",
            "Query: \\deftab720\n",
            "Rank 1: Document Great_Dane.txt, Score: 0.8180\n",
            "Rank 2: Document Labrador.txt, Score: 0.8015\n",
            "Rank 3: Document Beagle.txt, Score: 0.7857\n",
            "\n",
            "Query: \\pard\\pardeftab720\\partightenfactor0\n",
            "Rank 1: Document Great_Dane.txt, Score: 0.5474\n",
            "Rank 2: Document Labrador.txt, Score: 0.5149\n",
            "Rank 3: Document Beagle.txt, Score: 0.4849\n",
            "\n",
            "Query: \n",
            "Rank 1: Document Labrador.txt, Score: 1.0000\n",
            "Rank 2: Document German.txt, Score: 1.0000\n",
            "Rank 3: Document Beagle.txt, Score: 1.0000\n",
            "\n",
            "Query: \\f0\\fs24 \\cf0 \\expnd0\\expndtw0\\kerning0\n",
            "Rank 1: Document Great_Dane.txt, Score: 0.2996\n",
            "Rank 2: Document Labrador.txt, Score: 0.2651\n",
            "Rank 3: Document Beagle.txt, Score: 0.2352\n",
            "\n",
            "Query: \\outl0\\strokewidth0 \\strokec2 large breed with\\\n",
            "Rank 1: Document Pomeranian.txt, Score: 0.0032\n",
            "Rank 2: Document Rottweiler.txt, Score: 0.0029\n",
            "Rank 3: Document German.txt, Score: 0.0027\n",
            "\n",
            "Query: to find the perfect\\\n",
            "Rank 1: Document Golden.txt, Score: 0.0638\n",
            "Rank 2: Document Bulldog.txt, Score: 0.0413\n",
            "Rank 3: Document Husky.txt, Score: 0.0270\n",
            "\n",
            "Query: dog that is\\\n",
            "Rank 1: Document Husky.txt, Score: 0.0054\n",
            "Rank 2: Document Doberman.txt, Score: 0.0054\n",
            "Rank 3: Document Shiba.txt, Score: 0.0049\n",
            "\n",
            "Query: coat type and\\\n",
            "Rank 1: Document Dalmatian.txt, Score: 0.0657\n",
            "Rank 2: Document Golden.txt, Score: 0.0449\n",
            "Rank 3: Document Husky.txt, Score: 0.0427\n",
            "\n",
            "Query: suitable for\\\n",
            "Rank 1: Document Poodle.txt, Score: 0.4182\n",
            "Rank 2: Document Bulldog.txt, Score: 0.1755\n",
            "Rank 3: Document Boston.txt, Score: 0.1200\n",
            "\n",
            "Query: known for its\\\n",
            "Rank 1: Document Australian_Shepherd.txt, Score: 0.0061\n",
            "Rank 2: Document Boston.txt, Score: 0.0057\n",
            "Rank 3: Document Poodle.txt, Score: 0.0050\n",
            "\n",
            "Query: hypoallergenic and\\\n",
            "Rank 1: Document Golden.txt, Score: 0.4339\n",
            "Rank 2: Document Rottweiler.txt, Score: 0.4058\n",
            "Rank 3: Document Doberman.txt, Score: 0.3888\n",
            "\n",
            "Query: exercise needs of\\\n",
            "Rank 1: Document Dachshund.txt, Score: 0.0112\n",
            "Rank 2: Document Golden.txt, Score: 0.0067\n",
            "Rank 3: Document German.txt, Score: 0.0058\n",
            "\n",
            "Query: family-friendly and\\\n",
            "Rank 1: Document Golden.txt, Score: 0.0679\n",
            "Rank 2: Document Pomeranian.txt, Score: 0.0157\n",
            "Rank 3: Document Cavalier.txt, Score: 0.0140\n",
            "\n",
            "Query: breed with high\\\n",
            "Rank 1: Document Yorkshire.txt, Score: 0.0048\n",
            "Rank 2: Document Shiba.txt, Score: 0.0045\n",
            "Rank 3: Document Husky.txt, Score: 0.0045\n",
            "\n",
            "Query: a dog that is\\\n",
            "Rank 1: Document Doberman.txt, Score: 0.0029\n",
            "Rank 2: Document Shiba.txt, Score: 0.0022\n",
            "Rank 3: Document Husky.txt, Score: 0.0016\n",
            "\n",
            "Query: shedding and grooming\\\n",
            "Rank 1: Document Dalmatian.txt, Score: 0.0076\n",
            "Rank 2: Document Poodle.txt, Score: 0.0076\n",
            "Rank 3: Document Australian_Shepherd.txt, Score: 0.0066\n",
            "\n",
            "Query: who are looking for\\\n",
            "Rank 1: Document Pomeranian.txt, Score: 0.0101\n",
            "Rank 2: Document Dachshund.txt, Score: 0.0085\n",
            "Rank 3: Document Husky.txt, Score: 0.0076\n",
            "\n",
            "Query: easy to train\\\n",
            "Rank 1: Document Great_Dane.txt, Score: 0.0762\n",
            "Rank 2: Document Doberman.txt, Score: 0.0756\n",
            "Rank 3: Document Rottweiler.txt, Score: 0.0744\n",
            "\n",
            "Query: adapt well to\\\n",
            "Rank 1: Document Golden.txt, Score: 0.0713\n",
            "Rank 2: Document Husky.txt, Score: 0.0618\n",
            "Rank 3: Document German.txt, Score: 0.0475\n",
            "\n",
            "Query: known for their\\\n",
            "Rank 1: Document Boston.txt, Score: 0.0194\n",
            "Rank 2: Document Husky.txt, Score: 0.0179\n",
            "Rank 3: Document Dachshund.txt, Score: 0.0178\n",
            "\n",
            "Query: temperament suitable for\\\n",
            "Rank 1: Document Poodle.txt, Score: 0.1617\n",
            "Rank 2: Document Bulldog.txt, Score: 0.0659\n",
            "Rank 3: Document Great_Dane.txt, Score: 0.0594\n",
            "\n",
            "Query: small dog breeds that\\\n",
            "Rank 1: Document Dachshund.txt, Score: 0.0010\n",
            "Rank 2: Document Pomeranian.txt, Score: 0.0008\n",
            "Rank 3: Document Yorkshire.txt, Score: 0.0006\n",
            "\n",
            "Query: large dog breed\\\n",
            "Rank 1: Document Doberman.txt, Score: 0.0080\n",
            "Rank 2: Document Rottweiler.txt, Score: 0.0068\n",
            "Rank 3: Document Husky.txt, Score: 0.0064\n",
            "\n",
            "Query: companionship and loyalty\\\n",
            "Rank 1: Document Doberman.txt, Score: 0.0130\n",
            "Rank 2: Document German.txt, Score: 0.0122\n",
            "Rank 3: Document Rottweiler.txt, Score: 0.0092\n",
            "\n",
            "Query: }\n",
            "Rank 1: Document Labrador.txt, Score: 1.0000\n",
            "Rank 2: Document German.txt, Score: 1.0000\n",
            "Rank 3: Document Beagle.txt, Score: 1.0000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O7HnTxvZbHY6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}